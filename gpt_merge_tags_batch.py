import argparse
import os
import time
from typing import List, Sequence

from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm


# === åŸºç¡€é…ç½® ===
load_dotenv()

API_KEY = os.environ.get("OPENAI_API_KEY")
BASE_URL = os.environ.get("OPENAI_BASE_URL")

DEFAULT_MODEL_NAME = os.environ.get("E621_TAGGER_MODEL", "gpt-4o-mini")
DEFAULT_INPUT_DIR = os.environ.get(
    "E621_TAGGER_INPUT_DIR", r"E:\AI\my_project\e621-tagger\data\poren"
)
DEFAULT_MAX_RETRIES = 3
DEFAULT_DELAY_BETWEEN_CALLS = 1.0
DEFAULT_OUTPUT_SUFFIX = ".final.txt"
DEFAULT_TAG_SOURCES: List[str] = [".e621.txt", ".txt"]

DEFAULT_PROMPT_TEMPLATE = """You are a dataset refinement assistant specialized in tag cleaning for Stable Diffusion LoRA training.

Your task is to merge, clean, and optimize the provided tag lists.
Produce a compact, meaningful set of 15-25 tags for each image, suitable for LoRA training.

Rules:
1. Merge all tag lists and remove duplicates or synonyms.
2. Keep only visually or semantically meaningful tags:
   - species (e.g., dragon, furry, wolf)
   - anatomy (horns, wings, tail, scales)
   - gender or body shape (male, female, muscular)
   - clothing / armor / accessories
   - colors and materials (blue body, silver armor)
   - pose and composition (standing, solo, full body)
3. Remove the following:
   - background or environment tags (simple background, white background)
   - camera/view tags (looking at viewer, perspective)
   - redundant or generic terms (clothing, bottomwear, creature)
   - brand or franchise tags (e.g., yu-gi-oh!, konami, pokemon, digimon, disney, sega, nintendo)
   - specific character names unless the dataset is focused on that OC
4. Merge similar color tags (e.g. "blue scales", "blue fur" -> "blue body").
5. Use only lowercase English words, comma-separated.
6. Keep between 15-25 tags total.
7. Output ONLY the final tag list (no explanation).

{tag_sections}
"""


# === å‚æ•°è§£æ ===
def parse_tag_source(value: str) -> str:
    """åªæ¥æ”¶åç¼€å"""
    suffix = value.strip()
    if not suffix:
        raise argparse.ArgumentTypeError("Tag source suffix cannot be empty")
    if not suffix.startswith("."):
        suffix = f".{suffix}"
    return suffix


def normalize_suffix(suffix: str) -> str:
    suffix = suffix.strip()
    if not suffix:
        raise argparse.ArgumentTypeError("Suffix cannot be empty")
    if not suffix.startswith("."):
        suffix = f".{suffix}"
    return suffix


def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Merge tag files with GPT assistance")
    parser.add_argument(
        "-i",
        "--input-dir",
        default=DEFAULT_INPUT_DIR,
        help="Directory containing tag files (default: %(default)s)",
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        help="Directory to save merged tag files (default: same as input)",
    )
    parser.add_argument(
        "--tag-source",
        dest="tag_sources",
        action="append",
        type=parse_tag_source,
        help="Tag source suffix (e.g., .e621.txt). Repeat for multiple sources.",
    )
    parser.add_argument(
        "--output-suffix",
        default=DEFAULT_OUTPUT_SUFFIX,
        help="Suffix for merged output files (default: %(default)s)",
    )
    parser.add_argument(
        "--prompt-file",
        help="Path to a custom prompt template file. Use {tag_sections} as placeholder for inputs.",
    )
    parser.add_argument(
        "--prompt-extra",
        action="append",
        dest="prompt_extra",
        help="Additional instruction line appended to the prompt. Repeat to add more.",
    )
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL_NAME,
        help="OpenAI model name (default: %(default)s)",
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=DEFAULT_MAX_RETRIES,
        help="Maximum retry attempts on API failure (default: %(default)s)",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=DEFAULT_DELAY_BETWEEN_CALLS,
        help="Delay in seconds between successful API calls (default: %(default)s)",
    )
    parser.add_argument(
        "--log-file",
        help="Path to log file (default: <output-dir>/merge_log.txt)",
    )
    parser.add_argument(
        "--api-key",
        default=API_KEY,
        help="OpenAI API key (default: value from OPENAI_API_KEY)",
    )
    parser.add_argument(
        "--base-url",
        default=BASE_URL,
        help="Custom OpenAI API base URL (default: value from OPENAI_BASE_URL)",
    )

    args = parser.parse_args()
    args.output_suffix = normalize_suffix(args.output_suffix)
    return args


# === å·¥å…·å‡½æ•° ===
def load_prompt_template(path: str | None, extra_lines: Sequence[str] | None) -> str:
    if path:
        with open(path, "r", encoding="utf-8") as file:
            template = file.read()
    else:
        template = DEFAULT_PROMPT_TEMPLATE

    if extra_lines:
        template = f"{template.rstrip()}\n\n" + "\n".join(extra_lines)

    return template


def build_prompt(tag_lists: Sequence[str], template: str) -> str:
    tag_sections = "\n".join(f"Tags: {tags}" for tags in tag_lists if tags.strip())
    if "{tag_sections}" in template:
        return template.replace("{tag_sections}", tag_sections)
    return f"{template.rstrip()}\n\n{tag_sections}"


def create_client(api_key: str | None, base_url: str | None) -> OpenAI:
    client_kwargs = {}
    if api_key:
        client_kwargs["api_key"] = api_key
    if base_url:
        client_kwargs["base_url"] = base_url
    return OpenAI(**client_kwargs)


def gpt_merge_tags(client: OpenAI, model: str, prompt: str, max_retries: int) -> str:
    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            response = client.responses.create(model=model, input=prompt, temperature=0.2)
            return response.output_text.strip()
        except Exception as exc:
            last_error = exc
            print(f"âš ï¸ GPT è°ƒç”¨å¤±è´¥ ({attempt}/{max_retries})ï¼š{exc}")
            if attempt < max_retries:
                time.sleep(2 * attempt)
    if last_error:
        raise last_error
    raise RuntimeError("Unable to call GPT and no error captured")


# === ä¸»é€»è¾‘ ===
def main() -> None:
    args = parse_arguments()

    if not args.api_key and not os.environ.get("OPENAI_API_KEY"):
        raise RuntimeError("ç¼ºå°‘ OpenAI API keyï¼Œè¯·é€šè¿‡ --api-key æˆ–ç¯å¢ƒå˜é‡ OPENAI_API_KEY æä¾›")

    input_dir = os.path.abspath(args.input_dir)
    output_dir = os.path.abspath(args.output_dir) if args.output_dir else input_dir

    tag_sources = args.tag_sources or list(DEFAULT_TAG_SOURCES)
    if not tag_sources:
        raise RuntimeError("è‡³å°‘éœ€è¦ä¸€ä¸ªæ ‡ç­¾æ¥æº (--tag-source)")

    os.makedirs(output_dir, exist_ok=True)

    log_file = (
        os.path.abspath(args.log_file)
        if args.log_file
        else os.path.join(output_dir, "merge_log.txt")
    )

    client = create_client(args.api_key, args.base_url)
    prompt_template = load_prompt_template(args.prompt_file, args.prompt_extra)

    # === åªä¿ç•™å”¯ä¸€ base_nameï¼Œé˜²æ­¢é‡å¤å¤„ç† ===
    suffixes = tuple(tag_sources)
    all_files = [f for f in os.listdir(input_dir) if f.endswith(suffixes)]

    base_names = set()
    for f in all_files:
        for suffix in suffixes:
            if f.endswith(suffix):
                base_names.add(f[:-len(suffix)])
                break
    base_names = sorted(base_names)

    if not base_names:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ä»¥ {suffixes} ç»“å°¾çš„æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ç›®å½•è·¯å¾„ï¼š{input_dir}")
        return

    with open(log_file, "w", encoding="utf-8") as log:
        log.write("=== GPT æ ‡ç­¾èåˆæ—¥å¿— ===\n")
        log.write(f"è¾“å…¥ç›®å½•ï¼š{input_dir}\n")
        log.write(f"è¾“å‡ºç›®å½•ï¼š{output_dir}\n")
        log.write(f"ä½¿ç”¨æ¨¡å‹ï¼š{args.model}\n")
        log.write("æ ‡ç­¾æ¥æºï¼š" + ", ".join(tag_sources) + "\n\n")

    print(f"ğŸ—‚ è¾“å…¥æ–‡ä»¶å¤¹ï¼š{input_dir}")
    print(f"ğŸ—ƒ è¾“å‡ºæ–‡ä»¶å¤¹ï¼š{output_dir}")
    print(f"ğŸ”– æ ‡ç­¾æ¥æºåç¼€ï¼š{', '.join(tag_sources)}")
    print(f"ğŸš€ å…±æ£€æµ‹åˆ° {len(base_names)} ä¸ªåŸºç¡€æ–‡ä»¶ï¼Œå°†é€ä¸€èåˆæ ‡ç­¾...\n")

    for index, base_name in enumerate(
        tqdm(base_names, desc="Processing", ncols=100, dynamic_ncols=True), start=1
    ):
        tag_lists: List[str] = []
        missing_sources: List[str] = []

        for suffix in tag_sources:
            source_path = os.path.join(input_dir, f"{base_name}{suffix}")
            if not os.path.exists(source_path):
                missing_sources.append(source_path)
                continue
            with open(source_path, "r", encoding="utf-8") as source_file:
                tags = source_file.read().strip()
            tag_lists.append(tags)

        if missing_sources:
            tqdm.write(f"âš ï¸ {base_name} ç¼ºå°‘æ–‡ä»¶ï¼š{', '.join(missing_sources)}ï¼Œå·²è·³è¿‡")
            continue

        try:
            prompt = build_prompt(tag_lists, prompt_template)
            merged_tags = gpt_merge_tags(client, args.model, prompt, args.max_retries)
            tqdm.write(f"{base_name}_tags: {merged_tags}\n")

            output_path = os.path.join(output_dir, f"{base_name}{args.output_suffix}")
            with open(output_path, "w", encoding="utf-8") as out_file:
                out_file.write(merged_tags)

            tag_count = len([tag for tag in (t.strip() for t in merged_tags.split(",")) if tag])
            with open(log_file, "a", encoding="utf-8", buffering=1) as log:
                log.write(f"[OK] {base_name}{args.output_suffix} ({tag_count} tags)\n")
                log.flush()

            time.sleep(max(args.delay, 0.0))

        except Exception as exc:
            tqdm.write(f"âš ï¸ å¤„ç† {base_name} å‡ºé”™: {exc}")

    print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³è¾“å‡ºç›®å½•ã€‚")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")


if __name__ == "__main__":
    main()
